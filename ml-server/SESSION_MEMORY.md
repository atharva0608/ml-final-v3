# ML Server - Session Memory & Documentation

## üìã Overview
**Component**: ML Server (Machine Learning & Decision Engine)
**Purpose**: Handles ML model loading, predictions, and intelligent decision-making for Kubernetes cost optimization
**Instance Type**: Standalone server (can run on separate instance)
**Created**: 2025-11-28
**Last Updated**: 2025-11-28

---

## üéØ Core Responsibilities

### 1. ML Model Management
- Load and serve trained ML models for:
  - Spot instance interruption prediction
  - Resource utilization forecasting
  - Cost optimization recommendations
- Support model versioning and hot-reloading
- Handle model training data gap filling

### 2. Decision Engine (Pluggable Architecture)
- **Spot Optimizer Engine**: Selects optimal Spot instances based on:
  - Public AWS Spot Advisor data
  - Real-time spot price history
  - Historical interruption patterns
  - Time-of-day risk analysis
- **Bin Packing Engine**: Consolidates workloads to minimize node count
- **Rightsizing Engine**: Matches instance sizes to actual workload requirements
- **Office Hours Scheduler**: Auto-scales dev/staging environments

### 3. Data Processing
- **Gap Filler**: Handles scenario where model trained on old data needs recent data
  - Queries AWS APIs for missing data
  - Fills gaps from last training date to current deployment
  - Supports 15-day lookback requirement

---

## üîå Integration Points (Common Components)

### A. Communication with Central Server
**Protocol**: REST API + WebSocket (for real-time updates)
**Endpoints**:
- `POST /api/v1/ml/predict` - Receive prediction requests from Central Server
- `POST /api/v1/ml/decision` - Receive decision engine requests
- `GET /api/v1/ml/health` - Health check endpoint
- `WS /api/v1/ml/stream` - Real-time predictions stream

**Data Flow**:
```
Central Server ‚Üí ML Server: Request for decision/prediction
ML Server ‚Üí Central Server: Decision output with recommendations
```

### B. Data Exchange Format (COMMON SCHEMA)
```json
{
  "request_id": "uuid",
  "timestamp": "ISO-8601",
  "cluster_id": "customer-cluster-id",
  "request_type": "spot_selection|bin_packing|rightsizing",
  "input_data": {
    "current_state": {},
    "requirements": {},
    "constraints": {}
  }
}
```

**Response Format**:
```json
{
  "request_id": "uuid",
  "timestamp": "ISO-8601",
  "decision_type": "spot_instance_selection",
  "recommendations": [],
  "confidence_score": 0.85,
  "estimated_savings": 1250.50,
  "risk_assessment": {},
  "execution_plan": []
}
```

### C. Shared Configuration
**Location**: `/config/common.yaml`
**Contains**:
- Central Server connection details
- Database credentials (read-only access)
- AWS IAM role ARN
- Redis cache connection
- Logging configuration

### D. Database Access
**Type**: Read-only access to Central Server database
**Purpose**:
- Fetch historical cluster metrics
- Retrieve customer configuration
- Access training data for model updates

**Tables Used**:
- `clusters` - Cluster metadata
- `spot_history` - Historical spot interruptions
- `metrics_timeseries` - Cluster performance metrics
- `customer_config` - Customer-specific settings

---

## üìÅ Directory Structure

```
ml-server/
‚îú‚îÄ‚îÄ SESSION_MEMORY.md          # This file - session context & updates
‚îú‚îÄ‚îÄ README.md                   # Setup and deployment instructions
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ common.yaml            # Shared config with other servers
‚îÇ   ‚îú‚îÄ‚îÄ ml_config.yaml         # ML-specific configuration
‚îÇ   ‚îî‚îÄ‚îÄ models_registry.json   # Model versioning and paths
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ spot_predictor.py      # Spot interruption predictor
‚îÇ   ‚îú‚îÄ‚îÄ resource_forecaster.py # Resource usage forecasting
‚îÇ   ‚îî‚îÄ‚îÄ saved/                  # Trained model files
‚îÇ       ‚îú‚îÄ‚îÄ spot_predictor_v1.model
‚îÇ       ‚îî‚îÄ‚îÄ spot_predictor_v1_encoders.pkl
‚îú‚îÄ‚îÄ decision_engine/
‚îÇ   ‚îú‚îÄ‚îÄ base_engine.py         # Base class for all engines
‚îÇ   ‚îú‚îÄ‚îÄ spot_optimizer.py      # Spot instance selection engine
‚îÇ   ‚îú‚îÄ‚îÄ bin_packing.py         # Workload consolidation engine
‚îÇ   ‚îú‚îÄ‚îÄ rightsizing.py         # Instance rightsizing engine
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py            # Office hours scheduler
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ gap_filler.py          # Fills training data gaps
‚îÇ   ‚îú‚îÄ‚îÄ aws_fetcher.py         # Fetches data from AWS APIs
‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py        # Data preprocessing utilities
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ server.py              # FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictions.py     # Prediction endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decisions.py       # Decision engine endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py          # Health check endpoints
‚îÇ   ‚îî‚îÄ‚îÄ middleware/
‚îÇ       ‚îú‚îÄ‚îÄ auth.py            # Authentication middleware
‚îÇ       ‚îî‚îÄ‚îÄ logging.py         # Request logging
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ install.sh             # Installation script
‚îÇ   ‚îú‚îÄ‚îÄ start_server.sh        # Server startup script
‚îÇ   ‚îú‚îÄ‚îÄ train_models.sh        # Model training script
‚îÇ   ‚îî‚îÄ‚îÄ fill_data_gaps.sh      # Data gap filling script
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ test_decision_engines.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ API_SPEC.md            # API documentation
    ‚îî‚îÄ‚îÄ DECISION_ENGINES.md    # Decision engine algorithms
```

---

## üîß Technology Stack

### Core Framework
- **Language**: Python 3.10+
- **API Framework**: FastAPI 0.103+
- **ASGI Server**: Uvicorn

### ML Libraries
- **XGBoost**: 1.7.6 - Spot interruption prediction
- **scikit-learn**: 1.3.0 - Data preprocessing, evaluation
- **TensorFlow**: 2.13.0 - Deep learning models (future)
- **pandas**: 2.0.3 - Data manipulation
- **numpy**: 1.24.3 - Numerical operations

### AWS Integration
- **boto3**: 1.28+ - AWS SDK for Python
- **botocore**: 1.31+ - AWS core library

### Caching & Storage
- **Redis**: 5.0+ - Cache for Spot Advisor data, pricing
- **PostgreSQL**: Read-only client for Central DB

### Monitoring
- **prometheus-client**: Metrics export
- **python-json-logger**: Structured logging

---

## üöÄ Deployment Configuration

### Environment Variables
```bash
# Server Configuration
ML_SERVER_HOST=0.0.0.0
ML_SERVER_PORT=8001
ML_SERVER_WORKERS=4

# Central Server Connection
CENTRAL_SERVER_URL=http://central-server:8000
CENTRAL_SERVER_API_KEY=xxx

# Database (Read-Only)
DB_HOST=central-db.internal
DB_PORT=5432
DB_NAME=cloudoptim
DB_USER=ml_server_ro
DB_PASSWORD=xxx

# Redis Cache
REDIS_HOST=redis.internal
REDIS_PORT=6379
REDIS_DB=0

# AWS Configuration
AWS_REGION=us-east-1
AWS_ROLE_ARN=arn:aws:iam::xxx:role/MLServerRole

# Model Configuration
MODEL_DIR=/app/models/saved
MODEL_VERSION=v1
AUTO_RELOAD_MODELS=true
```

### Docker Configuration
```yaml
# docker-compose.yml
services:
  ml-server:
    build: ./ml-server
    ports:
      - "8001:8001"
    environment:
      - ML_SERVER_PORT=8001
    volumes:
      - ./ml-server/models/saved:/app/models/saved
      - ./ml-server/config:/app/config
    depends_on:
      - redis
    networks:
      - cloudoptim-network
```

---

## üìä Key Algorithms & Decision Logic

### 1. Spot Risk Score Calculation
**Formula**:
```
Risk Score = (0.60 √ó Public_Rate_Score) +
             (0.25 √ó Volatility_Score) +
             (0.10 √ó Gap_Score) +
             (0.05 √ó Time_Score)
```

**Thresholds**:
- Score > 0.65: Safe to use
- Score 0.40-0.65: Use with caution
- Score < 0.40: Avoid

### 2. Diversity Strategy
**Rule**: Never allocate >40% of nodes to single instance family
**Implementation**: `_apply_diversity_strategy()` in spot_optimizer.py

### 3. Data Gap Filling Logic
**Problem**: Model trained on data up to 30 days ago, need 15 days recent data
**Solution**:
1. Identify gap: `last_training_date` to `current_date`
2. Query AWS APIs for missing spot prices
3. Simulate/estimate missing interruption events
4. Merge with existing training data
5. Update model with fresh data

---

## üîÑ Session Updates Log

### 2025-11-28 - Initial Setup
**Changes Made**:
- Created ml-server folder structure
- Implemented base decision engine architecture
- Created SpotOptimizerEngine with risk scoring
- Implemented SpotInterruptionPredictor ML model
- Created DataGapFiller for handling training data gaps
- Defined common integration points with Central Server
- Documented data exchange formats

**Files Created**:
- `decision_engine/base_engine.py`
- `decision_engine/spot_optimizer.py`
- `models/spot_predictor.py`
- `data/gap_filler.py`
- `requirements.txt`

**Next Steps**:
1. Implement remaining decision engines (bin packing, rightsizing)
2. Create FastAPI server with prediction endpoints
3. Add Redis caching for Spot Advisor data
4. Create training pipeline script
5. Add comprehensive tests

---

## üîó Common Components Shared Across Servers

### 1. Authentication System
**Location**: Shared library (to be created)
**Used By**: All three servers
**Purpose**: Validate API keys, JWT tokens

### 2. Data Models (Pydantic Schemas)
**Location**: `/common/models.py` (to be created)
**Shared Schemas**:
- `ClusterState`
- `DecisionRequest`
- `DecisionResponse`
- `CustomerConfig`
- `MetricsData`

### 3. Database Schema
**Owner**: Central Server
**Accessed By**: All servers (ML: read-only, Client: read/write via API)
**Key Tables**:
- `customers` - Customer accounts
- `clusters` - Kubernetes clusters
- `nodes` - Cluster nodes
- `spot_events` - Spot interruption events
- `optimization_history` - Decision history

### 4. Message Queue (Future)
**Type**: RabbitMQ or Redis Pub/Sub
**Purpose**: Real-time event streaming between servers
**Events**:
- `spot_interruption_detected`
- `optimization_recommended`
- `cluster_state_changed`

### 5. Configuration Management
**Format**: YAML files
**Structure**:
```yaml
# common.yaml (shared by all servers)
environment: production
log_level: INFO
database:
  host: central-db.internal
  port: 5432
redis:
  host: redis.internal
  port: 6379
```

---

## üìù API Specifications

### Prediction Endpoint
```http
POST /api/v1/ml/predict/spot-interruption
Content-Type: application/json

{
  "instance_type": "m5.large",
  "region": "us-east-1",
  "availability_zone": "us-east-1a",
  "spot_price": 0.045,
  "launch_time": "2025-11-28T10:00:00Z"
}

Response:
{
  "interruption_probability": 0.08,
  "confidence": 0.92,
  "recommendation": "SAFE_TO_USE"
}
```

### Decision Engine Endpoint
```http
POST /api/v1/ml/decision/spot-optimize
Content-Type: application/json

{
  "cluster_id": "cluster-123",
  "requirements": {
    "cpu_required": 2.0,
    "memory_required": 8.0,
    "node_count": 10,
    "region": "us-east-1"
  }
}

Response:
{
  "decision_type": "spot_instance_selection",
  "recommendations": [...],
  "estimated_savings": 1250.50,
  "execution_plan": [...]
}
```

---

## üêõ Troubleshooting

### Model Loading Fails
**Symptom**: "Model file not found" error
**Solution**: Check MODEL_DIR path, ensure models are mounted correctly

### Low Prediction Accuracy
**Symptom**: Confidence scores < 0.70
**Solution**: Run data gap filler, retrain model with recent data

### High Response Latency
**Symptom**: API response time > 2 seconds
**Solution**: Enable Redis caching, increase workers, check DB connection

---

## üìå Important Notes

1. **Pluggable Architecture**: All decision engines inherit from `BaseDecisionEngine`
2. **Fixed Input/Output**: Standard `DecisionInput` and `DecisionOutput` contracts
3. **Agentless**: This server doesn't deploy to customer clusters
4. **Read-Only DB**: ML server has read-only access to Central DB
5. **Model Versioning**: Support multiple model versions, hot-reload capability

---

## üéØ Integration Checklist

- [ ] Central Server API endpoint configured
- [ ] Database read-only credentials set
- [ ] Redis cache connection tested
- [ ] AWS IAM role configured
- [ ] Common data schemas aligned
- [ ] Authentication middleware implemented
- [ ] Health check endpoint responding
- [ ] Logging forwarding to Central Server
- [ ] Model files deployed and loaded
- [ ] Data gap filler tested

---

**END OF SESSION MEMORY - ML SERVER**
*Append all future changes and updates below this line*

---

## üîÑ Session Updates Log (Continued)

### 2025-11-28 - Architecture Update: Inference-Only ML Server

**CRITICAL ARCHITECTURE CHANGE**: ML Server is now **inference and experimentation only**

#### Key Changes:

**1. No Training on ML Server** ‚ùå
- This server **does NOT train or retrain models**
- All training happens offline / elsewhere:
  - Separate training pipelines
  - Jupyter notebooks
  - Dedicated training infrastructure
- Once trained, models are **exported and uploaded** to this server

**2. Model Upload via Frontend** ‚úÖ
- Models and decision engines are **uploaded through ML frontend**
- Use **existing frontend design and layout** (same look & feel as current app)
- Only backend endpoints and wiring change
- Features:
  - Model upload UI (`.pkl` files, serialized models)
  - Decision engine upload/selection
  - Model versioning (A/B testing different versions)
  - Experimentation with new decision engines

**3. Automatic Gap-Filling (October ‚Üí Today Problem)** üîß

**The Problem**:
```
Model trained on data up to October
Instance needs predictions using October ‚Üí current date
Previously required manual data engineering
```

**The Solution** (On ML Server):
1. ML server knows model's `trained_until` date (stored in model metadata)
2. On startup or via ML frontend trigger:
   - Detects gap between `trained_until` and "today"
   - **Directly pulls historic market data on the same server**:
     - Spot/On-Demand prices for all instance types
     - Prices for all regions
     - Required metrics
   - Fills gap with historic prices + feature engineering
3. Once complete:
   - Model immediately produces **up-to-date predictions**
   - No waiting for weeks of new data collection

**Result**: As soon as instance starts/refreshes, get "today-ready" predictions using uploaded model + auto gap-filling

**4. Live Predictions & Decision Streaming** üìä

**Live Predictions**:
- After gap filled, ML server:
  - Continuously runs inference with:
    - Fresh incoming data
    - Up-to-date historic context (already filled)
  - Stores predictions in local store (DB/cache)
  - Optimized for time series plots and quick lookups

**Live Decisions**:
- Decision engine is **pluggable** and uploaded like models
- Fixed input format (normalized metrics, prices, states)
- Fixed output format (actions, scores, explanations)
- ML server:
  - Feeds predictions ‚Üí decision engine
  - Produces **live, actionable decisions**
  - Examples: "move to Spot in region X", "consolidate nodes", "rightsizing"
  - Exposes via APIs consumed by central backend & dashboards

**5. Frontend Features** (Using Current Design):
- **Keep current frontend design** (layout, styling, UX)
- New functionality:
  - ‚úÖ Model upload UI
  - ‚úÖ Decision engine upload/selection (dropdown for version)
  - ‚úÖ Gap-fill trigger & status display ("Fill missing data from 2025-10-01 to today")
  - ‚úÖ Live charts:
    - Predictions vs actuals (per instance/region)
    - Live decision stream visualized as timelines/markers/event overlays
  - All graphs update in near real-time with same visual style

#### Repository Layout Update

**Folder Structure Change**:
```
/old app/          # Legacy codebase (all existing files)
  ‚îú‚îÄ <existing frontend>
  ‚îú‚îÄ <existing backend>
  ‚îú‚îÄ <Dockerfiles, configs, scripts>
  ‚îî‚îÄ memory.md (old references)

/new app/          # New architecture
  ‚îú‚îÄ ml-server/           # Dedicated ML + decision server
  ‚îú‚îÄ core-platform/       # Central backend, DB, admin frontend
  ‚îú‚îÄ client-agent/        # Lightweight client-side agent
  ‚îú‚îÄ memory.md            # Updated architecture (this approach)
  ‚îî‚îÄ infra/               # docker-compose, IaC, scripts
```

#### Updated ML Server Responsibilities

**What ML Server DOES**:
- ‚úÖ Host serialized ML models (uploaded, not trained here)
- ‚úÖ Host pluggable decision engine modules
- ‚úÖ Serve model upload endpoints via frontend
- ‚úÖ Automatic gap-filling using historic prices (same server)
- ‚úÖ Run inference continuously
- ‚úÖ Stream live predictions
- ‚úÖ Execute decision engines
- ‚úÖ Expose APIs for predictions & decisions

**What ML Server DOES NOT DO**:
- ‚ùå Train or retrain models
- ‚ùå Heavy data engineering
- ‚ùå Long-term metric storage (that's central platform)

#### New API Endpoints

```http
# Model Management
POST /api/v1/ml/models/upload
  ‚Üí Upload trained model file
  ‚Üí Body: multipart/form-data with .pkl file
  ‚Üí Metadata: model_name, version, trained_until_date

GET /api/v1/ml/models/list
  ‚Üí List all uploaded models
  ‚Üí Returns: [{model_id, name, version, trained_until, uploaded_at}]

POST /api/v1/ml/models/activate
  ‚Üí Set active model version
  ‚Üí Body: {model_id, version}

# Decision Engine Management
POST /api/v1/ml/engines/upload
  ‚Üí Upload decision engine module
  ‚Üí Body: Python module file

GET /api/v1/ml/engines/list
  ‚Üí List available decision engines

POST /api/v1/ml/engines/select
  ‚Üí Select active decision engine
  ‚Üí Body: {engine_id, config}

# Gap Filling
POST /api/v1/ml/gap-filler/analyze
  ‚Üí Analyze data gaps for active model
  ‚Üí Returns: {trained_until, current_date, gap_days, required_data_types}

POST /api/v1/ml/gap-filler/fill
  ‚Üí Trigger automatic gap filling
  ‚Üí Pulls historic prices from AWS
  ‚Üí Returns: {status, records_filled, duration}

GET /api/v1/ml/gap-filler/status
  ‚Üí Check gap-filling progress
  ‚Üí Returns: {in_progress, percent_complete, eta}
```

#### Updated Environment Variables

```bash
# Model Configuration
MODEL_UPLOAD_DIR=/app/models/uploaded
MODEL_ACTIVE_VERSION=v1
ALLOW_MODEL_TRAINING=false  # Explicitly disabled

# Gap Filling Configuration
GAP_FILLER_ENABLED=true
GAP_FILLER_AWS_REGION=us-east-1
GAP_FILLER_INSTANCE_TYPES=m5.large,m5.xlarge,c5.large
GAP_FILLER_REGIONS=us-east-1,us-west-2,eu-west-1
GAP_FILLER_HISTORIC_DAYS_MAX=90

# Decision Engine Configuration
DECISION_ENGINE_DIR=/app/engines
DECISION_ENGINE_ACTIVE=spot_optimizer_v1
```

#### Migration Plan

**Phase 1** (Current):
- ‚úÖ Documentation updated
- ‚è≥ Waiting for user approval to implement

**Phase 2** (Implementation):
1. Move existing code to `/old app/`
2. Create `/new app/` structure
3. Implement model upload endpoints
4. Implement gap-filler with AWS price fetching
5. Create ML frontend (reuse current design)
6. Add decision engine upload capability

**Phase 3** (Testing):
1. Test model upload flow
2. Test gap-filling with real AWS data
3. Verify predictions after gap fill
4. Test decision engine swapping

---
