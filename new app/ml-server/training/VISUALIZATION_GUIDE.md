# ML Training Visualization Guide

This guide explains the 5 comprehensive visualizations generated by the Mumbai Spot Price Predictor and how they help you understand model accuracy and backtesting performance.

## üöÄ How to Run the Script

### In Jupyter Notebook (Recommended)

**Step 1**: Navigate to the training directory in your first cell:
```python
import os
os.chdir('/Users/atharvapudale/path/to/ml-final-v3/new app/ml-server/training')
print(f"Current directory: {os.getcwd()}")
```

**Step 2**: Run the script in the next cell:
```python
%run mumbai_price_predictor.py
```

**Alternative**: If you prefer to run cells individually, just ensure your Jupyter notebook is in the same directory as `visualization_insights.py` and `mumbai_price_predictor.py`.

### In Terminal

```bash
cd "new app/ml-server/training"
python3 mumbai_price_predictor.py
```

### ‚ö†Ô∏è Troubleshooting Import Errors

If you see `ModuleNotFoundError: No module named 'visualization_insights'`:

1. **Check your current directory**:
   ```python
   import os
   print(os.getcwd())
   ```

2. **Verify the visualization_insights.py file exists**:
   ```python
   import os
   print(os.path.exists('visualization_insights.py'))
   ```

3. **If False, navigate to the training directory**:
   ```python
   os.chdir('/full/path/to/ml-final-v3/new app/ml-server/training')
   ```

The script now automatically detects the correct directory, but Jupyter notebooks must be in the same directory as the visualization module.

---

## Overview

The visualization module creates **5 professional dashboards** with **meaningful, actionable insights** for:
- ‚úÖ **Model Accuracy Assessment** - How well the model predicts prices
- ‚úÖ **Backtesting Validation** - How predictions perform on 2025 test data
- ‚úÖ **Business Decision Making** - Which pools to choose for maximum savings with minimal risk

---

## üìä Visualization 1: Price Prediction Comparison

**File**: `price_prediction_comparison.png`

### What It Shows:
- **Predicted vs Actual Prices** for the top 4 safest pools (lowest risk score)
- Daily sampling over the entire 2025 test period (Q1-Q3)
- Statistical metrics for each pool:
  - **Average Price** - Mean spot price over the test period
  - **MAE (Mean Absolute Error)** - Average prediction error in dollars
  - **MAPE (Mean Absolute Percentage Error)** - Average prediction error as percentage
  - **Volatility** - Standard deviation of prices (price stability)

### Why It's Useful for Model Accuracy:
1. **Visual Confirmation** - You can see how closely predicted prices (dashed line) follow actual prices (solid line)
2. **Error Quantification** - MAE shows the dollar amount of average prediction error
3. **Percentage Accuracy** - MAPE shows prediction accuracy (lower is better)
   - MAPE < 5% = Excellent
   - MAPE < 10% = Good
   - MAPE > 20% = Needs improvement
4. **Price Range Context** - Shaded area shows ¬±1 standard deviation to understand volatility

### What to Look For:
- ‚úÖ **Tight overlap** between predicted and actual lines = good model
- ‚úÖ **Low MAE** (< $0.001 for spot prices) = accurate predictions
- ‚úÖ **Low MAPE** (< 10%) = consistent accuracy across all price levels
- ‚ùå **Wide gaps** or **systematic bias** = model needs retraining

---

## üéØ Visualization 2: Risk & Stability Dashboard

**File**: `risk_stability_dashboard.png`

### What It Shows (4 Panels):

#### Panel 1: Investment Matrix (Risk vs Reward)
- **X-axis**: Risk Score (0 = safe, 1 = risky)
- **Y-axis**: Annual Savings (USD)
- **Color**: Price Volatility (7-day standard deviation)
- **Quadrants**:
  - **BEST** - Low risk, High savings (green zone)
  - **HIGH REWARD** - High risk, High savings (orange zone)

#### Panel 2: Stability Score Ranking
- Horizontal bar chart showing stability scores for all 12 pools
- **Green** = Stable (> 0.67)
- **Orange** = Moderate (0.33 - 0.67)
- **Red** = Unstable (< 0.33)

#### Panel 3: Spot Discount by Instance Type
- Average discount percentage across all availability zones
- Error bars show min/max discount variation
- Helps identify which instance types offer the best savings

#### Panel 4: Annual Savings Potential
- Total annual savings per pool (per instance running 24/7)
- Color-coded by risk level:
  - **Green** = Low risk (< 0.4)
  - **Orange** = Medium risk (0.4 - 0.6)
  - **Red** = High risk (> 0.6)

### Why It's Useful for Backtesting:
1. **Real-World Validation** - Uses actual 2025 data to calculate risk and savings
2. **Business Decision Support** - Shows which pools maximize ROI with acceptable risk
3. **Cross-Pool Comparison** - Identifies best-performing pools across all metrics
4. **Risk-Adjusted Returns** - Balances savings potential with stability

### What to Look For:
- ‚úÖ **Pools in the "BEST" quadrant** = ideal choices (low risk, high savings)
- ‚úÖ **High stability scores** (> 0.6) = reliable pools
- ‚úÖ **Consistent discounts** across AZs = predictable savings
- ‚ö†Ô∏è **High volatility + high risk** = avoid or use with caution

---

## üìà Visualization 3: Price Trend Analysis

**File**: `price_trend_analysis.png`

### What It Shows:
- **Monthly price trends** for each of the 4 instance types
- **Average price line** (solid blue)
- **Price range** (min-max shaded area)
- **Trend line** (dashed red) showing upward or downward price movement
- **Trend percentage** - Overall price change from Q1 to Q3 2025

### Why It's Useful for Backtesting:
1. **Temporal Patterns** - Reveals if prices are trending up/down over time
2. **Seasonality Detection** - Shows monthly variation patterns
3. **Volatility Over Time** - Shaded area shows how price stability changes
4. **Long-Term Predictions** - Validates if model captures trend direction

### What to Look For:
- ‚úÖ **Stable trends** (flat or slight slope) = predictable prices
- ‚úÖ **Narrow price range** = low volatility = safer pools
- ‚ö†Ô∏è **Sharp upward trends** = prices increasing (lower future savings)
- ‚ö†Ô∏è **Wide price range** = high volatility = risky pools

---

## ü§ñ Visualization 4: Model Performance Dashboard

**File**: `model_performance_dashboard.png`

### What It Shows (5 Panels):

#### Panel 1: Top 15 Feature Importance
- **Most influential features** for price prediction
- Helps understand what drives spot price changes
- Expected top features:
  - `instance_type_encoded` - Different instance types have different base prices
  - `on_demand_price` - Spot prices correlate with on-demand prices
  - `price_max_24h`, `price_mean_24h` - Recent price history matters
  - `discount` - Historical discount patterns continue

#### Panel 2: Prediction Error Distribution
- **Histogram** of prediction errors (predicted - actual)
- **Red line** at 0 = perfect predictions
- **Green line** = mean error (should be near 0)
- **Bell curve shape** = normally distributed errors (good)

#### Panel 3: Model Metrics Comparison
- **MAE** and **RMSE** for train vs test sets
- **Green bars** = training set
- **Orange bars** = test set
- Test metrics should be close to train metrics (no overfitting)

#### Panel 4: R¬≤ Score Visualization
- **R¬≤ (coefficient of determination)** measures variance explained
- **Target**: 0.85 (our model should aim for this)
- **Interpretation**:
  - R¬≤ > 0.6 = Good (model explains 60%+ of price variance)
  - R¬≤ > 0.4 = Moderate
  - R¬≤ < 0.4 = Poor (needs retraining)

#### Panel 5: MAPE Comparison
- **Mean Absolute Percentage Error** for train vs test
- **Target**: < 15%
- **Interpretation**:
  - MAPE < 10% = Excellent accuracy
  - MAPE < 20% = Good accuracy
  - MAPE > 20% = Poor accuracy

### Why It's THE MOST IMPORTANT Visualization for Model Accuracy:
1. **Comprehensive Metrics** - Shows all key ML performance indicators
2. **Train vs Test Comparison** - Detects overfitting (if test metrics are much worse)
3. **Error Analysis** - Distribution shows if errors are systematic or random
4. **Feature Insights** - Understand what the model learned
5. **Quantitative Validation** - Hard numbers to judge model quality

### What to Look For:
- ‚úÖ **Test R¬≤ > 0.6** = model is accurate
- ‚úÖ **Test MAPE < 10%** = excellent predictions
- ‚úÖ **Test metrics ‚âà Train metrics** = no overfitting
- ‚úÖ **Bell-shaped error distribution centered at 0** = unbiased predictions
- ‚ùå **Large gap between train and test R¬≤** = overfitting (model memorized training data)
- ‚ùå **Skewed error distribution** = systematic bias (model consistently over/under-predicts)

---

## üìã Visualization 5: Executive Summary

**File**: `summary_insights.png`

### What It Shows (5 Panels):

#### Panel 1: Recommended Pool
- **Best pool** based on lowest risk score
- Shows: Annual savings, Discount %, Stability, Risk
- **Why this pool?** - Bullet points explaining the recommendation

#### Panel 2: Risk Distribution Pie Chart
- Breakdown of pools by risk level:
  - **Low Risk** (0.0 - 0.3) - Green
  - **Medium Risk** (0.3 - 0.6) - Orange
  - **High Risk** (0.6 - 1.0) - Red
- Shows percentage of pools in each category

#### Panel 3: Total Savings Potential
- **Sum of annual savings** across all 12 pools
- **Average discount** percentage
- **Top saver** pool identification

#### Panel 4: Model Performance Summary
- **Test R¬≤**, **Test MAE**, **Test MAPE**
- **Interpretation** with pass/fail indicators:
  - ‚úÖ Good
  - ‚ö†Ô∏è Moderate
  - ‚ùå Needs improvement

#### Panel 5: Instance Type Comparison
- **Grouped bar chart** comparing 4 instance types on:
  - **Savings** (annual, scaled to $100s)
  - **Discount** (%)
  - **Stability** (%)
  - **Safety** (1 - risk_score, %)
- Helps identify best instance type overall

### Why It's Useful for Decision Making:
1. **Actionable Recommendation** - Direct answer: "Use this pool"
2. **Risk Summary** - Quick view of overall pool safety
3. **ROI Quantification** - Total savings potential
4. **Model Confidence** - Pass/fail on model quality
5. **Instance Type Guidance** - Which instance type family is best

### What to Look For:
- ‚úÖ **Recommended pool with high stability** (> 0.6) = safe choice
- ‚úÖ **Model performance = "Good"** = trust the recommendations
- ‚úÖ **Most pools in Low/Medium risk** = plenty of safe options
- ‚ö†Ô∏è **Model performance = "Moderate/Needs improvement"** = be cautious with recommendations

---

## üéØ Summary: How to Validate Model Accuracy & Backtesting

### Quick Model Accuracy Checklist:

1. **Check Visualization 4 (Model Performance Dashboard)**:
   - ‚úÖ Test R¬≤ > 0.6 (preferably > 0.8)
   - ‚úÖ Test MAPE < 10% (preferably < 5%)
   - ‚úÖ Test metrics within 20% of train metrics (no overfitting)
   - ‚úÖ Error distribution centered at 0 (no bias)

2. **Check Visualization 1 (Price Prediction Comparison)**:
   - ‚úÖ Predicted lines closely follow actual lines
   - ‚úÖ MAE < $0.001 for spot prices (< $0.01 for on-demand)
   - ‚úÖ MAPE < 10% across all pools

3. **Check Visualization 3 (Price Trend Analysis)**:
   - ‚úÖ Model captures trend direction (upward/downward)
   - ‚úÖ Predicted prices stay within ¬±1 std dev of actual

### Quick Backtesting Validation Checklist:

1. **Check Visualization 2 (Risk & Stability Dashboard)**:
   - ‚úÖ At least 3-4 pools in "BEST" quadrant (low risk, high savings)
   - ‚úÖ Stability scores > 0.5 for recommended pools
   - ‚úÖ Savings estimates are realistic (not too good to be true)

2. **Check Visualization 5 (Executive Summary)**:
   - ‚úÖ Recommended pool has stability > 0.6
   - ‚úÖ Risk distribution shows most pools are Low/Medium risk
   - ‚úÖ Model performance = "Good"

3. **Real-World Validation**:
   - ‚úÖ 2025 Q1-Q3 test data represents actual future conditions
   - ‚úÖ All 4 instance types tested (t3.medium, t4g.medium, c5.large, t4g.small)
   - ‚úÖ All 3 availability zones tested (aps1-az1, aps1-az2, aps1-az3)

---

## üìù Column Usage Verification

### ‚úÖ Columns USED by the Model:
- `instance_type` - Encoded to numeric for ML
- `availability_zone` - Used for pool grouping
- `spot_price` - Target variable (what we predict)
- `on_demand_price` - Used as feature (reference price)
- `region` - Filter for ap-south-1 (Mumbai)
- `timestamp` - Used for time-based features and sorting
- `savings` - Calculated and used for analysis
- `discount` - Calculated and used for risk scoring

### ‚ùå Columns NOT USED (Dropped):
- `sps` - Unused artifact, dropped
- `if` - Unused artifact, dropped
- `sourcefile` - Unused artifact, dropped
- `t3` - Unused artifact, dropped
- `t2` - Unused artifact, dropped

### ‚úÖ Instance Types Used:
- `t3.medium` - General purpose, burstable
- `t4g.medium` - ARM-based, cost-optimized
- `c5.large` - Compute-optimized
- `t4g.small` - ARM-based, smallest

---

## üîß Technical Details

### Data Flow:
1. **Load Data** ‚Üí 2023-2024 training, 2025 Q1-Q3 test
2. **Clean Data** ‚Üí Drop unnecessary columns (sps, if, sourcefile, t3, t2)
3. **Feature Engineering** ‚Üí Create 73 features from 8 base columns
4. **Train Model** ‚Üí XGBoost on 2023-2024 data
5. **Backtest** ‚Üí Predict on 2025 data
6. **Score Pools** ‚Üí Calculate risk & stability for 12 pools
7. **Visualize** ‚Üí Generate 5 comprehensive dashboards

### Visualization Files Generated:
1. `price_prediction_comparison.png` (18x12 inches, 300 DPI)
2. `risk_stability_dashboard.png` (18x12 inches, 300 DPI)
3. `price_trend_analysis.png` (18x12 inches, 300 DPI)
4. `model_performance_dashboard.png` (18x12 inches, 300 DPI)
5. `summary_insights.png` (18x10 inches, 300 DPI)

All saved to: `./training/outputs/`

---

## üöÄ Next Steps

After reviewing all visualizations:

1. **If Model Performance is Good** (R¬≤ > 0.6, MAPE < 10%):
   - ‚úÖ Trust the recommendations
   - ‚úÖ Use recommended pool for production
   - ‚úÖ Monitor actual spot prices vs predictions
   - ‚úÖ Retrain monthly with new data

2. **If Model Performance is Moderate** (R¬≤ 0.4-0.6, MAPE 10-20%):
   - ‚ö†Ô∏è Use with caution
   - ‚ö†Ô∏è Consider adding more features
   - ‚ö†Ô∏è Increase lookback period
   - ‚ö†Ô∏è Try different models (LightGBM, Neural Network)

3. **If Model Performance is Poor** (R¬≤ < 0.4, MAPE > 20%):
   - ‚ùå Do NOT use for production
   - ‚ùå Debug feature engineering
   - ‚ùå Check for data quality issues
   - ‚ùå Retrain with different hyperparameters

---

## üìû Support

For questions about the visualizations or model performance, refer to:
- `training/README.md` - Training script overview
- `training/DATA_FORMAT.md` - Expected data format
- `models/PricePrediction.py` - Production model implementation

---

**Generated by**: CloudOptim ML Server Training Pipeline
**Last Updated**: 2025-11-30
